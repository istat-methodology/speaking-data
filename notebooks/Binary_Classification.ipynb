{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤– Binary Classification with a Neural Network\n",
        "\n",
        "In this exercise, weâ€™ll simulate a real-world scenario:\n",
        "\n",
        "> **Can we predict whether a student will pass a course based on the time spent studying?**\n",
        "\n",
        "Weâ€™ll build a neural network to tackle this problem using just two inputs:\n",
        "\n",
        "- `Lecture Hours`: how much time a student spent attending lectures  \n",
        "- `Project Hours`: how much time was invested in project work\n",
        "\n",
        "---\n",
        "\n",
        "### What youâ€™ll learn:\n",
        "\n",
        "âœ… How to generate synthetic data  \n",
        "âœ… How to train a neural network for binary classification  \n",
        "âœ… What **binary cross-entropy** means and why we use it  \n",
        "âœ… How to **visualize the decision boundary** of a model  \n",
        "âœ… And most importantly... how neural networks \"think\" ðŸ‘€\n",
        "\n",
        "Ready? Letâ€™s dive in! ðŸš€\n"
      ],
      "metadata": {
        "id": "nDLZ4uP3zd9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "9tR_tLCwnlF-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVewSEAYeQz8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras import models, layers, Input"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“Š Generate a Sample Dataset\n",
        "\n",
        "In this section, we simulate a simple dataset representing students preparing for an exam.  \n",
        "Each student is characterized by:\n",
        "\n",
        "- The number of **lecture hours** attended.\n",
        "- The number of **project hours** completed.\n",
        "\n",
        "We assume that both features influence the probability of passing the course.  \n",
        "The pass/fail outcome is then generated using a **logistic model**, which is commonly used for binary classification tasks.\n"
      ],
      "metadata": {
        "id": "5PjbNhPZnnF6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of students\n",
        "n_students = 200\n",
        "\n",
        "# Input features\n",
        "lecture_hours = np.random.normal(30, 5, n_students)\n",
        "project_hours = np.random.normal(20, 3, n_students)\n",
        "\n",
        "# Logistic model for pass probability\n",
        "z = (\n",
        "    0.65 * lecture_hours +\n",
        "    0.35 * project_hours -\n",
        "    26.5\n",
        ")\n",
        "prob_pass = 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Simulate pass/fail outcome\n",
        "pass_course = np.random.binomial(1, prob_pass)\n",
        "\n",
        "# Create dataframe\n",
        "df = pd.DataFrame({\n",
        "    'lecture_hours': lecture_hours.round(2),\n",
        "    'project_hours': project_hours.round(2),\n",
        "    'pass_course': pass_course\n",
        "})\n",
        "\n",
        "# Preview\n",
        "df.head()"
      ],
      "metadata": {
        "id": "wT92fANrnT_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pass_course'].value_counts()"
      ],
      "metadata": {
        "id": "j8p3CymJoLWQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Scatter Plot"
      ],
      "metadata": {
        "id": "DT-fz483zjxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Colors: red for 1 (PASS), blu for 0 (FAIL)\n",
        "colors = ['blue' if label == 0 else 'red' for label in df['pass_course']]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(df['lecture_hours'], df['project_hours'], c=colors, edgecolor='k')\n",
        "plt.xlabel(\"Lecture Hours\")\n",
        "plt.ylabel(\"Project Hours\")\n",
        "plt.title(\"Student Dataset\")\n",
        "plt.grid(True)\n",
        "\n",
        "# Legenda manuale\n",
        "import matplotlib.patches as mpatches\n",
        "legend_labels = [mpatches.Patch(color='red', label='Pass (1)'),\n",
        "                 mpatches.Patch(color='blue', label='Fail (0)')]\n",
        "plt.legend(handles=legend_labels)\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "xpMsJoA1wsjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§ª Train-Test Split and Feature Standardization\n",
        "\n",
        "To evaluate our model properly, we divide the dataset into two parts:\n",
        "\n",
        "- **Training set**: used to train the model.\n",
        "- **Test set**: used to assess how well the model generalizes to unseen data.\n",
        "\n",
        "We also **standardize the features** to ensure that both variables (lecture hours and project hours) are on the same scale.  \n",
        "This is important because many machine learning algorithms, including logistic regression, are sensitive to the magnitude of input values.\n"
      ],
      "metadata": {
        "id": "ccahHwRDpke5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split features and target\n",
        "X = df.drop(columns='pass_course')\n",
        "y = df['pass_course']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "zM-uzHv6o-Om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  Build the Neural Network\n",
        "\n",
        "Now we define a simple **Multilayer Perceptron (MLP)** to predict whether a student will pass the course.  \n",
        "This kind of neural network is made up of **fully connected layers** and is well-suited for tabular data like ours.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“ Network Architecture\n",
        "\n",
        "Our model includes:\n",
        "\n",
        "- **Input layer**: receives the two features (`lecture_hours` and `project_hours`).\n",
        "- **First hidden layer**: 16 neurons with ReLU activation.\n",
        "- **Second hidden layer**: 8 neurons, also with ReLU.\n",
        "- **Output layer**: 1 neuron with **sigmoid** activation, which outputs a probability between 0 and 1 (suitable for binary classification).\n",
        "\n",
        "---\n",
        "\n",
        "> ðŸ” **Why ReLU?**  \n",
        "> ReLU (Rectified Linear Unit) is a commonly used activation function that helps the network learn non-linear patterns efficiently.\n",
        "\n",
        "> ðŸŽ¯ **Why sigmoid at the output?**  \n",
        "> It compresses the output to a value between 0 and 1 â€” ideal when predicting probabilities in binary classification problems.\n"
      ],
      "metadata": {
        "id": "LSK8uJR1qUCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the MLP model\n",
        "model = models.Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
        "])"
      ],
      "metadata": {
        "id": "G9HvY7kTpaqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ“‰ Binary Cross Entropy\n",
        "\n",
        "To train our binary classifier, we use **Binary Cross Entropy (BCE)** as the loss function.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§® What is Binary Cross Entropy?\n",
        "\n",
        "Binary Cross Entropy measures the difference between the predicted probabilities and the actual class labels (0 or 1).  \n",
        "It is defined as:\n",
        "\n",
        "$\n",
        "\\text{BCE} = -\\left[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})\\right]\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $ y $ is the true label (0 or 1)\n",
        "- $ \\hat{y} $ is the predicted probability (between 0 and 1)\n",
        "\n",
        "---\n",
        "\n",
        "> ðŸ“Œ **Intuition**:  \n",
        "> BCE penalizes predictions that are far from the true label.  \n",
        "> For example, if the true label is 1 and the model predicts 0.01, the loss will be large.  \n",
        "> If it predicts 0.99, the loss will be small â€” meaning the model is doing well.\n",
        "\n",
        "> ðŸ” The goal of training is to minimize this loss over all training examples.\n",
        "\n"
      ],
      "metadata": {
        "id": "TWFe85TXqrwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "TQkzl67dpdiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ‹ï¸ Train the Model\n",
        "\n",
        "Now itâ€™s time to train our neural network!\n",
        "\n",
        "---\n",
        "\n",
        "### âš™ï¸ Model Compilation\n",
        "\n",
        "Before training, we **compile the model** by specifying:\n",
        "\n",
        "- **Loss function**: we use `Binary Crossentropy`, which is suitable for binary classification tasks.\n",
        "- **Optimizer**: `Adam` is a widely used optimizer that adapts the learning rate during training.\n",
        "- **Metrics**: we track `accuracy` to measure how often the model correctly predicts the outcome.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸš€ Fit the Model\n",
        "\n",
        "We train the model using the `.fit()` method, providing:\n",
        "\n",
        "- **Training data** (`X_train_scaled`, `y_train`)\n",
        "- **Validation data** (`X_test_scaled`, `y_test`) â€” useful to monitor generalization during training\n",
        "- **Epochs**: the number of full passes through the training data\n",
        "- **Batch size**: how many samples to process before updating weights\n",
        "\n",
        "> ðŸ“Œ Training will produce a history object containing accuracy and loss values for both training and validation sets â€” very helpful for diagnosing underfitting or overfitting.\n"
      ],
      "metadata": {
        "id": "8Qh-dXjKq30-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train_scaled, y_train, epochs=50, validation_data=(X_test_scaled, y_test), verbose=1)\n",
        "\n",
        "# Evaluate on test data\n",
        "loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "print(f\"Test accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "5uOS8t0VqcXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training & validation accuracy and loss\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train Acc')\n",
        "plt.title('Model Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "# Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "# plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZgkrYSYyrKDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸ§  How does the Neural Network make decisions?\n",
        "\n",
        "Once the model is trained, we can **visualize how it separates the two classes** â€” students predicted to pass vs those predicted to fail.\n",
        "\n",
        "The plot below shows:\n",
        "\n",
        "- The input space (`Lecture Hours` vs `Project Hours`)\n",
        "- A **colored background** based on the predicted outcome\n",
        "- The **decision boundary** (black line) where the model is exactly 50% confident\n",
        "\n",
        "> ðŸ§­ Try reading this plot like a map:  \n",
        "> Each dot is a student. Where they fall in the space determines the model's prediction.\n",
        "\n",
        "Can you spot the students that fall on the â€œwrongâ€ side? ðŸ˜‰\n"
      ],
      "metadata": {
        "id": "MJTidIeyzMKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Grid per le coordinate\n",
        "x_min, x_max = df['lecture_hours'].min() - 1, df['lecture_hours'].max() + 1\n",
        "y_min, y_max = df['project_hours'].min() - 1, df['project_hours'].max() + 1\n",
        "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300),\n",
        "                     np.linspace(y_min, y_max, 300))\n",
        "\n",
        "# Creazione DataFrame e scalatura\n",
        "grid_points = pd.DataFrame({\n",
        "    'lecture_hours': xx.ravel(),\n",
        "    'project_hours': yy.ravel()\n",
        "})\n",
        "grid_scaled = scaler.transform(grid_points)\n",
        "\n",
        "# Predizione del modello\n",
        "Z = model.predict(grid_scaled).reshape(xx.shape)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Colori di sfondo per le classi predette\n",
        "plt.contourf(xx, yy, Z > 0.5, alpha=0.4, cmap=plt.cm.RdBu)\n",
        "\n",
        "# Linea di contorno per probabilitÃ  = 0.5\n",
        "contours = plt.contour(xx, yy, Z, levels=[0.5], colors='black', linewidths=2)\n",
        "plt.clabel(contours, fmt={0.5: 'Decision Boundary'}, inline=True, fontsize=10)\n",
        "\n",
        "# Dati reali\n",
        "plt.scatter(df['lecture_hours'], df['project_hours'],\n",
        "            c=df['pass_course'], cmap=plt.cm.RdBu, edgecolor='k')\n",
        "\n",
        "plt.xlabel(\"Lecture Hours\")\n",
        "plt.ylabel(\"Project Hours\")\n",
        "plt.title(\"Neural Network Decision Boundary\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "SRB2gYw2xQzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test new student data (clean version)\n",
        "def predict_outcome(lecture_hours, project_hours):\n",
        "    input_df = pd.DataFrame([{\n",
        "        'lecture_hours': lecture_hours,\n",
        "        'project_hours': project_hours\n",
        "    }])\n",
        "\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "    prob = model.predict(input_scaled).flatten()[0]\n",
        "\n",
        "    print(f\"\\nPredicted probability of passing: {prob:.2f}\")\n",
        "    if prob >= 0.5:\n",
        "        print(\"Prediction: PASS\")\n",
        "    else:\n",
        "        print(\"Prediction: FAIL\")\n"
      ],
      "metadata": {
        "id": "PyFmEyBxtBA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LECTURE_HOURS = 18\n",
        "PROJECT_HOURS = 10\n",
        "\n",
        "# Try it out!\n",
        "predict_outcome(LECTURE_HOURS, PROJECT_HOURS)"
      ],
      "metadata": {
        "id": "7Q_pnrYYtFJ7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}